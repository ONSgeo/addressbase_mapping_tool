{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mapping tool for internal use\n",
    "#### - Maps point geometries\n",
    "#### - Builds choropleth maps of the frequency of those points within the boundaries of polygon geometries\n",
    "#### - Includes a reference layer showing the number of points per 1000 population\n",
    "\n",
    "This tool is designed for internal use to quickly highlight areas of interest for further analysis. \n",
    "It should be easy to drop data in and get something out!\n",
    "Because it doesn't assume that the geographies it's being fed have a standard output area code, the program uses spatial joins rather than NSUL lookups. As this means it may be slightly inaccurate especially around borders it shouldn't be relied on to produce public-facing statistics.\n",
    "\n",
    "By default it produces maps using the quartile scheme with 10 bins. This behaviour can be changed in the options in cell 2."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### By: Samantha Iacob (samantha.iacob@ons.gov.uk)\n",
    "\n",
    "Last edited: 25/6/2024\n",
    "\n",
    "To do: \n",
    "\n",
    "- Panel controls to allow quick adjusting what the choropleth maps are showing (longer term)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run cells 1-4 in order. Cells 5 and 6 are optional"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cell 1: Include variables (edit the values within the quotations to the paths to your .shp files within the\n",
    "data folder. Please fill in the other details as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#file variables (important)\n",
    "point_data_file = r\"post_offices_and_post_boxes_EW.csv\" #shp or csv file in data directory, converts csv to point geometry if needed\n",
    "point_data_classification_code_column_name = \"class_code\" #shouldn't need to change this normally\n",
    "point_data_classification_description_column_name = \"class_desc\" #shouldn't need to change this normally\n",
    "\n",
    "# polygon_data_file_1 should be the largest geography as it will set the default zoom\n",
    "\n",
    "polygon_data_file_1 = r\"MSOA_2021_EW_BGC_V2_493701222045311232\\MSOA_2021_EW_BGC_V2.shp\"\n",
    "polygon_data_file_1_description = \"England & Wales: MSOAs 2021\"\n",
    "polygon_data_file_1_short_description = \"England & Wales\"\n",
    "polygon_data_file_1_OA_code = \"MSOA21CD\"\n",
    "polygon_data_file_1_OA_name_override = \"\" #optional, this value is calculated from the OA code if left blank\n",
    "\n",
    "polygon_data_file_2 = r\"\"\n",
    "polygon_data_file_2_description = \"\"\n",
    "polygon_data_file_2_short_description = \"\"\n",
    "polygon_data_file_2_OA_code = \"\"\n",
    "polygon_data_file_2_OA_name_override = \"\" #optional, this value is calculated from the OA code if left blank\n",
    "\n",
    "polygon_data_file_3 = r\"\"\n",
    "polygon_data_file_3_description = \"\"\n",
    "polygon_data_file_3_short_description = \"\"\n",
    "polygon_data_file_3_OA_code = \"\"\n",
    "polygon_data_file_3_OA_name_override = \"\" #optional, this value is calculated from the OA code if left blank\n",
    "\n",
    "polygon_data_file_4 = r\"\"\n",
    "polygon_data_file_4_description = \"\"\n",
    "polygon_data_file_4_short_description = \"\"\n",
    "polygon_data_file_4_OA_code = \"\"\n",
    "polygon_data_file_4_OA_name_override = \"\" #optional, this value is calculated from the OA code if left blank"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cell 2: Optional settings, please adjust to your liking if you feel like it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#options (change if needed)\n",
    "\n",
    "#Set this to false to not generate a comparison map showing population based on 2021 LA boundaries. This saves a\n",
    "#little time and space if not needed\n",
    "LA_comparison = True\n",
    "\n",
    "#Save csv point data (if present) to a shapefile in output folder. Set to False to prevent this.\n",
    "save_point_shapefile = True\n",
    "\n",
    "#the colour of each classification's points on the map will be drawn from this list in order.\n",
    "#the colours were taken from here https://sashamaps.net/docs/resources/20-colors/ using the 99% accessible version\n",
    "#and removing red, green and yellow as they don't show up well against the colour maps I picked.\n",
    "\n",
    "colour_list = ['#4363d8',\n",
    " '#f032e6',\n",
    " '#469990',\n",
    " '#000075',\n",
    " '#800000',\n",
    " '#42d4f4',\n",
    " '#a9a9a9',\n",
    " '#dcbeff',\n",
    " '#911eb4',\n",
    " '#aaffc3',\n",
    " '#9A6324',\n",
    " '#fffac8',\n",
    " '#f58231',\n",
    " '#000000',\n",
    " '#ffffff']\n",
    "\n",
    "#The scheme and bin count used for the polygon geometry choropleth maps. \n",
    "#Available schemes : \n",
    "'''\n",
    "'boxplot', 'equalinterval', 'fisherjenks', 'fisherjenkssampled', \n",
    "'headtailbreaks', 'jenkscaspall', 'jenkscaspallforced', 'jenkscaspallsampled', \n",
    "'maxp', 'maximumbreaks', 'naturalbreaks', 'quantiles', 'percentiles', 'stdmean', 'userdefined'\n",
    "'''\n",
    "scheme = 'quantiles'\n",
    "k = 10\n",
    "\n",
    "#the colour map used for each polygon geometry set on the map. \n",
    "#https://matplotlib.org/stable/gallery/color/colormap_reference.html for more information\n",
    "colormaps = ['summer_r','autumn_r','winter_r','spring_r']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cell 3: The code that prepares the data for the map. Please run it but don't alter things in here unless necessary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#libraries\n",
    "import os\n",
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "import folium\n",
    "from folium.plugins import GroupedLayerControl\n",
    "from datetime import datetime\n",
    "from sys import exit\n",
    "\n",
    "#time and savefile variables\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "x = 1\n",
    "\n",
    "#functions for this tool\n",
    "def point_data_from_csv(point_data_file = point_data_file, timestamp = timestamp, point_data_classification_code_column_name = \"class_code\"):\n",
    "    '''\n",
    "    merges csv output from a SQL query with a classifications list then converts it to a geodataframe containing point\n",
    "    geometry based on the x and y columns in the data, and then saves it as a shape file in the outputs directory for future use.\n",
    "    This is only appropriate for UK as x and y are Easting and Northing values and are mapped to the EPSG:27700 CRS.\n",
    "    '''\n",
    "    #ingesting data\n",
    "    point_df = pd.read_csv('data/' + point_data_file)\n",
    "    classifications_df = pd.read_csv('data/addressbase-product-classification-scheme.csv')\n",
    "    #merging and arranging dataframes\n",
    "    point_classified_df = pd.merge(point_df, classifications_df, left_on = point_data_classification_code_column_name, right_on = \"Concatenated\", how = 'left', copy = False)\n",
    "    point_classified_df = point_classified_df[[\"uprn\", \"class_code\", \"Class_Desc\",\"x\", \"y\"]]\n",
    "    point_classified_df.rename(columns = {'Class_Desc': 'class_desc'}, inplace = True)\n",
    "    #converting merged dataframe to geodataframe\n",
    "    point_gdf = gpd.GeoDataFrame(point_classified_df, geometry = gpd.points_from_xy(point_classified_df.x, point_classified_df.y), crs = 'EPSG:27700')\n",
    "    if save_point_shapefile == True:\n",
    "        #exporting savefile\n",
    "        os.makedirs(f'outputs/{point_data_file[:-4]}_{timestamp}')\n",
    "        outfilepath = os.path.join('outputs',f'{point_data_file[:-4]}_{timestamp}/{point_data_file[:-4]}_{timestamp}.shp')\n",
    "        point_gdf.to_file(outfilepath)\n",
    "        print(f\"{point_data_file} shapefile saved to: outputs/{point_data_file[:-4]}_{timestamp}/{point_data_file[:-4]}_{timestamp}.shp\")\n",
    "    else:\n",
    "        print(f\"{point_data_file} shapefile not saved, set save_point_shapefile to True in Cell 2 to change this behaviour.\")\n",
    "    return point_gdf\n",
    "\n",
    "def add_aggregate_data_cols(polygon_data, point_data, output_area_col_name, classification_code_col_name = point_data_classification_code_column_name):\n",
    "    #inputs : a gdf with polygon geometry, a gdf with points geometry - intersecting, the names of the OA and classification code columns.\n",
    "    #Returns a geodataframe with polygon geometry and with counts of how many of each classification appears within those polygons.\n",
    "    point_polygon = gpd.sjoin(polygon_data, point_data, how = 'inner', predicate = 'intersects')\n",
    "    agg_df = point_polygon.groupby(by = [output_area_col_name, classification_code_col_name]).agg('count').reset_index()\n",
    "    \n",
    "    area_list = agg_df[output_area_col_name].unique().tolist()\n",
    "    class_list = agg_df[classification_code_col_name].unique().tolist()\n",
    "    \n",
    "    \n",
    "    new_df = pd.DataFrame(data = {output_area_col_name : area_list} )\n",
    "    \n",
    "    #creating a list of dataframes which consist of columns: OA code and count. one for each classification code in the point data \n",
    "    frame_of_frames = []\n",
    "    counter = 0                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             \n",
    "    for classification in class_list:\n",
    "        frame_of_frames.append(pd.merge(new_df, agg_df[agg_df[classification_code_col_name] == classification]))\n",
    "        frame_of_frames[counter].rename(columns = {'geometry' : classification}, inplace = True)\n",
    "        frame_of_frames[counter] = frame_of_frames[counter][[output_area_col_name, classification]]\n",
    "        counter +=1\n",
    "   \n",
    "    #smushing it together into a single df, and removing NANs                 \n",
    "    for frame in frame_of_frames:\n",
    "        new_df = pd.merge(new_df, frame, how = \"left\", copy = False)\n",
    "    new_df = new_df.fillna(0) \n",
    "\n",
    "    \n",
    "    #fixing dtypes and populating a total column\n",
    "    new_df['all'] = 0\n",
    "    for classification in class_list:\n",
    "        new_df[classification] = new_df[classification].astype('int32')\n",
    "        new_df['all'] += new_df[classification]\n",
    "        \n",
    "    #merging the classification counts with the dataframe that holds the polygon geometry\n",
    "    merged_gdf = pd.merge(polygon_data, new_df, how = 'left', on = output_area_col_name, copy = False)\n",
    "    return merged_gdf\n",
    "\n",
    "def classification_dictionary(point_data, classification_code_col_name = point_data_classification_code_column_name, class_description_code = point_data_classification_description_column_name):\n",
    "    #outputs a dictionary of classifications found in the point data gdf\n",
    "    class_list = classification_list(point_data, classification_code_col_name)\n",
    "    new_dict = {}\n",
    "    for item in class_list:\n",
    "        new_dict[item] = point_data[(point_data[classification_code_col_name] == item)][class_description_code].values[0]\n",
    "    return new_dict\n",
    "\n",
    "def classification_list(point_data, classification_code_col_name = point_data_classification_code_column_name):\n",
    "    #returns a list of unique classifications in the point data\n",
    "    new_list = point_data[classification_code_col_name].unique().tolist()\n",
    "    return new_list\n",
    "\n",
    "def classification_colour(classification_list):\n",
    "    #returns a dict of point data classifications and an associated colour for mapping\n",
    "        new_dict = {}\n",
    "        counter = 0\n",
    "        for item in classification_list:\n",
    "            new_dict[item] = colour_list[counter]\n",
    "            counter += 1\n",
    "        return new_dict\n",
    "    \n",
    "#importing data\n",
    "\n",
    "if (point_data_file[len(point_data_file)-3:]) == 'csv':\n",
    "    point_data = point_data_from_csv()\n",
    "elif (point_data_file[len(point_data_file)-3:]) == 'shp':\n",
    "    point_data = gpd.read_file(f\"data/{point_data_file}\")\n",
    "else:\n",
    "    exit(\"Invalid file type for point geometry. Must be .csv or .shp file located within the data directory of this tool.\")\n",
    "\n",
    "#as there can be more than one polygon dataset (england and wales geogs, etc, putting polygon data into a list of dfs)\n",
    "\n",
    "polygon_data = []\n",
    "polygon_data_desc = []\n",
    "polygon_data_short_desc = []\n",
    "polygon_OA_code = []\n",
    "polygon_OA_name = []\n",
    "\n",
    "if polygon_data_file_1 != \"\":\n",
    "    polygon_data.append(gpd.read_file(f\"data/{polygon_data_file_1}\"))\n",
    "    polygon_data_desc.append(polygon_data_file_1_description)\n",
    "    polygon_data_short_desc.append(polygon_data_file_1_short_description)\n",
    "    polygon_OA_code.append(polygon_data_file_1_OA_code)\n",
    "    if polygon_data_file_1_OA_name_override != \"\":\n",
    "        polygon_OA_name.append(polygon_data_file_1_OA_name_override)\n",
    "    else:\n",
    "        polygon_OA_name.append(polygon_data_file_1_OA_code[:-2] + \"NM\")\n",
    "        \n",
    "if polygon_data_file_2 != \"\":\n",
    "    polygon_data.append(gpd.read_file(f\"data/{polygon_data_file_2}\"))\n",
    "    polygon_data_desc.append(polygon_data_file_2_description)\n",
    "    polygon_data_short_desc.append(polygon_data_file_2_short_description)\n",
    "    polygon_OA_code.append(polygon_data_file_2_OA_code)\n",
    "    if polygon_data_file_2_OA_name_override != \"\":\n",
    "        polygon_OA_name.append(polygon_data_file_2_OA_name_override)\n",
    "    else:\n",
    "        polygon_OA_name.append(polygon_data_file_2_OA_code[:-2] + \"NM\")\n",
    "        \n",
    "if polygon_data_file_3 != \"\":\n",
    "    polygon_data.append(gpd.read_file(f\"data/{polygon_data_file_3}\"))\n",
    "    polygon_data_desc.append(polygon_data_file_3_description)\n",
    "    polygon_data_short_desc.append(polygon_data_file_3_short_description)\n",
    "    polygon_OA_code.append(polygon_data_file_3_OA_code)\n",
    "    if polygon_data_file_3_OA_name_override != \"\":\n",
    "        polygon_OA_name.append(polygon_data_file_3_OA_name_override)\n",
    "    else:\n",
    "        polygon_OA_name.append(polygon_data_file_3_OA_code[:-2] + \"NM\")\n",
    "\n",
    "if polygon_data_file_4 != \"\":\n",
    "    polygon_data.append(gpd.read_file(f\"data/{polygon_data_file_4}\"))\n",
    "    polygon_data_desc.append(polygon_data_file_4_description)\n",
    "    polygon_data_short_desc.append(polygon_data_file_4_short_description)\n",
    "    polygon_OA_code.append(polygon_data_file_4_OA_code)\n",
    "    if polygon_data_file_4_OA_name_override != \"\":\n",
    "        polygon_OA_name.append(polygon_data_file_4_OA_name_override)\n",
    "    else:\n",
    "        polygon_OA_name.append(polygon_data_file_4_OA_code[:-2] + \"NM\")       \n",
    "\n",
    "counter = 0\n",
    "for map in polygon_data:\n",
    "    polygon_data[counter] = add_aggregate_data_cols(polygon_data[counter], point_data,polygon_OA_code[counter])\n",
    "    counter +=1\n",
    "\n",
    "#populating classifications lists and dictionaries.\n",
    "class_list = classification_list(point_data)\n",
    "class_dict = classification_dictionary(point_data)\n",
    "colour_dict = classification_colour(class_list)\n",
    "\n",
    "#generating a population dataset of LAs which can be used as a comparator layer in the map\n",
    "#this information is found on the MYE3 tab of the local authority boundary edition of the mid 2021 population estimate\n",
    "# https://www.ons.gov.uk/peoplepopulationandcommunity/populationandmigration/populationestimates/datasets/estimatesofthepopulationforenglandandwales\n",
    "if LA_comparison == True:\n",
    "    \n",
    "    #ingesting data\n",
    "    res_data = gpd.read_file(\"data/Local_Authority_Districts_December_2021_GB_BGC_2022_7766355490887282253/LAD_DEC_2021_GB_BGC.shp\")\n",
    "    pop_data = pd.read_csv(\"data/population_e_w_2021.csv\")\n",
    "    res_data = pd.merge(res_data, pop_data, how = 'left', left_on = \"LAD21CD\",right_on = \"Code\", copy = False)[[\"LAD21CD\", \"LAD21NM\", \"BNG_E\", \"BNG_N\",\"LONG\", \"LAT\", \"GlobalID\", \"geometry\",\"Population\"]]\n",
    "    res_data = res_data[~res_data[\"LAD21CD\"].str.startswith(\"S\")] # removing Scotland. \n",
    "    res_data = add_aggregate_data_cols(res_data, point_data, \"LAD21CD\")\n",
    "    res_data['per_thousand'] = round((res_data['all']/(res_data['Population'])*1000),3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cell 4: run to generate a folium map in the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#note: depending on the complexity of the map it can take a short while to load, please be patient.\n",
    "\n",
    "#this is the target column for the cloropleth maps. If panel controls added later can be adjusted via the controls:\n",
    "column = 'all'\n",
    "\n",
    "#for styles to remain consistent between map elements the variables can be edited here\n",
    "\n",
    "poly_style_kwds = {\"fillOpacity\": 0.4,}\n",
    "poly_highlight_kwds = {\"fillOpacity\" : 0.5,}\n",
    "\n",
    "marker_style_kwds = {\"stroke\": True,\n",
    "                     \"opacity\" : 1,\n",
    "                     \"fillOpacity\" : 1,\n",
    "                    }\n",
    "marker_kwds = {\"fill\" : True,\n",
    "                \"radius\": 50,\n",
    "                }\n",
    "\n",
    "#generating the tooltips and aliases for the polygon geometry maps\n",
    "tooltips = []\n",
    "alias_list = []\n",
    "counter = 0\n",
    "for map in polygon_data:\n",
    "    tooltips.append([polygon_OA_name[counter], polygon_OA_code[counter]])\n",
    "    alias_list.append([polygon_OA_name[counter], polygon_OA_code[counter]])\n",
    "    for item in class_list:\n",
    "        tooltips[counter].append(item)\n",
    "        alias_list[counter].append(class_dict[item])\n",
    "    tooltips[counter].append('all')\n",
    "    alias_list[counter].append('All classifications')\n",
    "    counter += 1\n",
    "\n",
    "\n",
    "#rendering the largest geography first as it will set default zoom\n",
    "\n",
    "counter = 0\n",
    "for map in polygon_data:\n",
    "    if counter == 0:\n",
    "        m = polygon_data[counter].explore(column,\n",
    "                                          tiles = None,\n",
    "                                          name = polygon_data_desc[counter],\n",
    "                                          cmap = colormaps[counter],\n",
    "                                          scheme = scheme,\n",
    "                                          k = k,\n",
    "                                          style_kwds = poly_style_kwds,\n",
    "                                          highlight_kwds = poly_highlight_kwds,\n",
    "                                          legend_kwds = {\"caption\" : f\"{polygon_data_short_desc[counter]}, {column} classifications\",\n",
    "                                                        \"scale\" : False,\n",
    "                                                        },\n",
    "                                          tooltip = tooltips[counter],\n",
    "                                          tooltip_kwds = {\"aliases\" : alias_list[counter]},\n",
    "                                         )\n",
    "    else:\n",
    "        m = polygon_data[counter].explore(column,\n",
    "                                          name = polygon_data_desc[counter],\n",
    "                                          cmap = colormaps[counter],\n",
    "                                          scheme = scheme,\n",
    "                                          k = k,\n",
    "                                          style_kwds = poly_style_kwds,\n",
    "                                          highlight_kwds = poly_highlight_kwds,\n",
    "                                          legend_kwds = {\"caption\" : f\"{polygon_data_short_desc[counter]}, {column} classifications\",\n",
    "                                                        \"scale\" : False,\n",
    "                                                        },\n",
    "                                          tooltip = tooltips[counter],\n",
    "                                          tooltip_kwds = {\"aliases\" : alias_list[counter]},\n",
    "                                          m = m,\n",
    "                                          )\n",
    "    counter += 1\n",
    "    \n",
    "#population data (could be included with all maps using this tool - population as of 2021 using Census 21 LA bounds)\n",
    "\n",
    "if LA_comparison == True:\n",
    "    res_alias_list = ['LAD21NM', 'LAD21CD', 'Population']\n",
    "    res_tooltips= ['LAD21NM', 'LAD21CD', 'Population']\n",
    "    for item in class_list:\n",
    "        res_tooltips.append(item)\n",
    "        res_alias_list.append(class_dict[item])\n",
    "    res_tooltips.append('all')\n",
    "    res_tooltips.append('per_thousand')\n",
    "    res_alias_list.append('All classifications')\n",
    "    res_alias_list.append('All classifications per 1000 population')\n",
    "    \n",
    "    m = res_data.explore(\"per_thousand\",\n",
    "                     name = \"All classifications per 1000 ppl/LA, 2021\", \n",
    "                     cmap = 'inferno_r', \n",
    "                     scheme = 'quantiles',\n",
    "                     k = 10,\n",
    "                     style_kwds = {\"fillOpacity\": 0.9,},\n",
    "                     highlight_kwds = {\"fillOpacity\": 1.0,},\n",
    "                     legend_kwds = {\"caption\" : \"Observations per 1000 population, mid-2021\",\n",
    "                                                        \"scale\" : False,\n",
    "                                                        },\n",
    "                     tooltip = res_tooltips,\n",
    "                     tooltip_kwds = {\"aliases\" : res_alias_list},\n",
    "                     show = False,\n",
    "                     m = m,\n",
    "                    )\n",
    "\n",
    "#point data\n",
    "\n",
    "for classification in class_list:\n",
    "    point_data[point_data[point_data_classification_code_column_name] == classification].explore(\n",
    "        m=m,\n",
    "        name = class_dict[classification],\n",
    "        marker_type = \"circle\",\n",
    "        marker_kwds = marker_kwds,\n",
    "        style_kwds = marker_style_kwds,\n",
    "        color = colour_dict[classification],\n",
    "    )\n",
    "\n",
    "# adding selectable tilesets to the map. The last layer here will be the active one when the map is loaded\n",
    "\n",
    "folium.TileLayer('cartodbdarkmatter', name = 'Tiles: CartoDB Dark Matter').add_to(folium.FeatureGroup('tiles')).add_to(m)\n",
    "folium.TileLayer('cartodbpositron', name = 'Tiles: CartoDB Positron').add_to(folium.FeatureGroup('tiles')).add_to(m)\n",
    "\n",
    "#adding a control box element to the map \n",
    "\n",
    "folium.LayerControl().add_to(m)\n",
    "\n",
    "m #display the map"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cell 5: Run to show the map in a tab of your browser window. It will lock the tool while that window is open."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "m.show_in_browser()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cell 6: This will save your map as a html file in the 'outputs' directory of this tool. It can be opened in your browser."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m.save(f'outputs/{timestamp}_map_{x}.html')\n",
    "print(f\"map saved to: outputs/{timestamp}_map_{x}.html\")\n",
    "x += 1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "geopy38",
   "language": "python",
   "name": "geopy38"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
